import streamlit as st
import pandas as pd
import numpy as np
import spacy
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline
import time
import json
from collections import defaultdict
import re
import os
from io import BytesIO, StringIO
import base64
import zipfile
import tempfile
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import matplotlib
matplotlib.use('Agg')
from matplotlib.font_manager import FontProperties


# åˆå§‹åŒ–æ¨¡å‹
nlp = spacy.load("zh_core_web_sm")
sentiment_analyzer = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-jd-full-chinese")
ner_model = pipeline("ner", model="dslim/bert-base-NER")
plt.rcParams['font.sans-serif'] = ['SimHei']
# é¢„å®šä¹‰çŠ¯ç½ªå…³é”®è¯åº“
CRIME_KEYWORDS = {
    "æ¯’å“": ["ç™½ç²‰", "å†°", "å¶å­", "ç³–", "è´§", "ä¸œè¥¿", "å¿«é€’", "å†°å¦¹", "æºœå†°", "é£è¡Œ", "é£è¡Œå‘˜", "é£å¶å­", "é£è¡Œå‘˜",
             "ä¸œè¥¿"],
    "è¯ˆéª—": ["è½¬è´¦", "æ±‡æ¬¾", "é“¶è¡Œå¡", "å¯†ç ", "éªŒè¯ç ", "ä¸­å¥–", "ä¿è¯é‡‘", "æ‰‹ç»­è´¹", "è¿”åˆ©", "åˆ·å•", "æŠ•èµ„", "å½©ç¥¨",
             "èµŒåš"],
    "æš´åŠ›": ["ç ", "æ€", "æ‰“", "æ•™è®­", "æ”¶æ‹¾", "åºŸäº†", "åšäº†", "å¹²æ‰", "æŠ¥ä»‡", "ç®—è´¦", "è§è¡€", "å·¥å…·"],
    "èµŒåš": ["èµŒ", "ç‰Œ", "å±€", "åº„", "ä¸‹æ³¨", "æ¢­å“ˆ", "ç™¾å®¶ä¹", "é¾™è™", "èµŒèµ„", "æŠ½æ°´", "æ´—ç ", "ç›˜å£"],
    "èµ°ç§": ["è´§", "è¿‡å…³", "æ°´è·¯", "ä¸Šå²¸", "æ¸…å…³", "æŠ¥å…³", "é›†è£…ç®±", "ç å¤´", "æ¸”èˆ¹", "å¿«è‰‡", "è¾¹å¢ƒ", "é€šé“"],
    "æ¶‰é»‘": ["è€å¤§", "å¸®æ´¾", "å ‚å£", "æ”¶ä¿æŠ¤è´¹", "çœ‹åœºå­", "ç äºº", "ç«æ‹¼", "åœ°ç›˜"],
}

# é¢„å®šä¹‰çŠ¯ç½ªæš—è¯­æ˜ å°„
EUPHEMISMS = {
    "èŒ¶å¶": "æ¯’å“",
    "é’“é±¼": "èµ°ç§",
    "å¿«é€’": "æ¯’å“äº¤æ˜“",
    "æµ·é²œ": "èµ°ç§è´§ç‰©",
    "ç³–": "å†°æ¯’",
    "ä¸œè¥¿": "æ¯’å“",
    "è´§": "æ¯’å“",
    "æ”¶ç§Ÿ": "æ”¶èµŒå€º",
    "çœ‹æµ·": "èµ°ç§äº¤æ˜“",
    "å¼€å¼ ": "çŠ¯ç½ªæ´»åŠ¨å¼€å§‹",
    "è€åœ°æ–¹": "çŠ¯ç½ªäº¤æ˜“åœ°ç‚¹",
    "å¤©æ°”è½¬å‡‰": "æ¯’å“äº¤æ˜“æš—è¯­",
    "é’“é±¼": "èµ°ç§æ´»åŠ¨",
    "æµ·è¾¹": "èµ°ç§åœ°ç‚¹",
    "é€è´§": "æ¯’å“äº¤æ˜“",
    "æ¥è´§": "æ¯’å“äº¤æ˜“",
    "æ¸…æ´": "æ´—é’±",
    "å·¥ç¨‹": "éæ³•æ´»åŠ¨",
    "æ°´æœ": "èµƒç‰©",
}

# æ¡ˆä»¶ç±»å‹æ˜ å°„
CASE_TYPE_MAPPING = {
    "æ¯’å“": ["ç™½ç²‰", "å†°", "å¶å­", "ç³–", "è´§", "å¿«é€’", "æºœå†°"],
    "è¯ˆéª—": ["è½¬è´¦", "æ±‡æ¬¾", "é“¶è¡Œå¡", "å¯†ç ", "éªŒè¯ç ", "ä¸­å¥–", "ä¿è¯é‡‘"],
    "æš´åŠ›": ["ç ", "æ€", "æ‰“", "æ•™è®­", "æ”¶æ‹¾", "åºŸäº†", "åšäº†"],
    "èµŒåš": ["èµŒ", "ç‰Œ", "å±€", "åº„", "ä¸‹æ³¨", "æ¢­å“ˆ", "ç™¾å®¶ä¹"],
    "èµ°ç§": ["è¿‡å…³", "æ°´è·¯", "ä¸Šå²¸", "é›†è£…ç®±", "ç å¤´", "æ¸”èˆ¹", "è¾¹å¢ƒ"],
    "æ¶‰é»‘": ["è€å¤§", "å¸®æ´¾", "å ‚å£", "ä¿æŠ¤è´¹", "çœ‹åœºå­", "ç«æ‹¼", "åœ°ç›˜"],
}


# åˆå§‹åŒ–æ•°æ®
def load_sample_data():
    return [
        "é¾™å“¥ï¼šä»Šæ™šè€åœ°æ–¹äº¤è´§ï¼Œå¸¦å¤Ÿç°é‡‘",
        "å¼ ä¸‰ï¼šæ˜ç™½ï¼Œè¿™æ¬¡è´§çš„å“è´¨å¦‚ä½•ï¼Ÿ",
        "æå››ï¼šå¤©æ°”è½¬å‡‰äº†ï¼Œå¤šç©¿ç‚¹",
        "ç‹äº”ï¼šç›®æ ‡ä¸Šé’©äº†ï¼Œå·²è½¬è´¦5ä¸‡",
        "é¾™å“¥ï¼šæ”¶åˆ°ï¼Œåˆ†ä½ ä¸‰æˆ",
        "å¼ ä¸‰ï¼šä¸Šæ¬¡çš„'ç³–'çº¯åº¦ä¸å¤Ÿï¼Œå®¢æˆ·ä¸æ»¡æ„",
        "æå››ï¼šæ–°'å¿«é€’'æ˜å¤©åˆ°ï¼Œæ³¨æ„æŸ¥æ”¶",
        "ç‹äº”ï¼šè¿™æ¬¡å¿…é¡»å¹²ç¥¨å¤§çš„",
        "è·¯äººç”²ï¼šå‘¨æœ«ä¸€èµ·åƒé¥­å—ï¼Ÿ",
        "é¾™å“¥ï¼šä¸å®ˆè§„çŸ©çš„äººè¦å¥½å¥½æ•™è®­",
        "å¼ ä¸‰ï¼šè­¦å¯Ÿæœ€è¿‘æŸ¥å¾—ä¸¥ï¼Œå°å¿ƒç‚¹",
        "æå››ï¼šæƒ³ç»“æŸè¿™ç§ç”Ÿæ´»...",
        "ç‹äº”ï¼šæµ·è¾¹é’“é±¼è®¡åˆ’å–æ¶ˆï¼Œæ”¹ä¸‹å‘¨",
    ]


# å®ä½“è¯†åˆ«
def extract_entities(text):
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        if ent.label_ in ["PERSON", "ORG", "GPE"]:
            entities.append((ent.text, ent.label_))
    return entities


# æ•æ„Ÿè¯é¢˜åˆ†æ
def detect_sensitive_topics(text):
    topics = []
    for topic, keywords in CRIME_KEYWORDS.items():
        if any(keyword in text for keyword in keywords):
            topics.append(topic)
    return topics


# çŠ¯ç½ªæ„å›¾åˆ†æ
def analyze_criminal_intent(text):
    # æš—è¯­æ£€æµ‹
    for euphemism, meaning in EUPHEMISMS.items():
        if euphemism in text:
            return f"çŠ¯ç½ªæš—è¯­æ£€æµ‹: [{euphemism}] = {meaning}", 85

    # å…³é”®è¯åŒ¹é…
    for topic, keywords in CRIME_KEYWORDS.items():
        if any(keyword in text for keyword in keywords):
            return f"å…³é”®è¯è§¦å‘: {topic}", 90

    # æƒ…æ„Ÿå¢å¼ºåˆ†æ
    sentiment = sentiment_analyzer(text)[0]
    if sentiment['label'] == 'NEGATIVE' and sentiment['score'] > 0.9:
        return "é«˜è´Ÿé¢æƒ…ç»ª: " + text, 75

    return "æœªæ£€æµ‹åˆ°æ˜æ˜¾æ„å›¾", 5


# æƒ…æ„Ÿåˆ†æ
def analyze_sentiment(text):
    result = sentiment_analyzer(text)[0]
    return result['label'], result['score']


# æ„å»ºå…³ç³»å›¾è°±
def build_relationship_graph(messages):
    G = nx.DiGraph()
    entity_messages = defaultdict(list)

    # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰å®ä½“
    for msg in messages:
        entities = extract_entities(msg)
        for entity, label in entities:
            G.add_node(entity, type=label)

    # ç¬¬äºŒéï¼šå»ºç«‹å®ä½“å…³ç³»
    for i, msg in enumerate(messages):
        entities = [e[0] for e in extract_entities(msg)]
        for j in range(len(entities)):
            for k in range(j + 1, len(entities)):
                if G.has_edge(entities[j], entities[k]):
                    G[entities[j]][entities[k]]['weight'] += 1
                else:
                    G.add_edge(entities[j], entities[k], weight=1)
                entity_messages[(entities[j], entities[k])].append(msg)

    # è®¡ç®—èŠ‚ç‚¹å½±å“åŠ›
    centrality = nx.degree_centrality(G)
    for node in G.nodes:
        G.nodes[node]['centrality'] = centrality.get(node, 0)

    return G, entity_messages


# èšç±»åˆ†æ
def cluster_messages(messages, n_clusters=4):
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(messages)
    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)

    clusters = {}
    for i, label in enumerate(kmeans.labels_):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(messages[i])

    return clusters


# é«˜äº®æ˜¾ç¤ºå…³é”®ç‰‡æ®µ
def highlight_keywords(text):
    # é«˜äº®å®ä½“
    entities = extract_entities(text)
    for entity, label in entities:
        if label == "PERSON":
            text = text.replace(entity,
                                f"<span style='background-color: #ffb6c1; padding: 2px; border-radius: 3px;'>{entity}</span>")
        elif label == "ORG":
            text = text.replace(entity,
                                f"<span style='background-color: #add8e6; padding: 2px; border-radius: 3px;'>{entity}</span>")
        elif label == "GPE":
            text = text.replace(entity,
                                f"<span style='background-color: #98fb98; padding: 2px; border-radius: 3px;'>{entity}</span>")

    # é«˜äº®çŠ¯ç½ªå…³é”®è¯
    for topic, keywords in CRIME_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text:
                if topic == "æ¯’å“":
                    color = "#ff9999"
                elif topic == "è¯ˆéª—":
                    color = "#ffcc99"
                elif topic == "æš´åŠ›":
                    color = "#ff6666"
                elif topic == "èµŒåš":
                    color = "#ccccff"
                elif topic == "èµ°ç§":
                    color = "#99ccff"
                else:  # æ¶‰é»‘
                    color = "#d9b3ff"
                text = text.replace(keyword,
                                    f"<span style='background-color: {color}; padding: 2px; border-radius: 3px; font-weight: bold;'>{keyword}</span>")

    # é«˜äº®çŠ¯ç½ªæš—è¯­
    for euphemism in EUPHEMISMS.keys():
        if euphemism in text:
            text = text.replace(euphemism,
                                f"<span style='background-color: #ffff99; padding: 2px; border-radius: 3px; font-weight: bold; border: 1px dashed #ff9900;'>{euphemism}</span>")

    return text


# æ–‡ä»¶å†…å®¹åˆ†æ
def analyze_file_content(content):
    # å°è¯•è§£ææ–‡ä»¶å†…å®¹
    try:
        # å°è¯•è§£æä¸ºJSON
        data = json.loads(content)
        if isinstance(data, list):
            return data
        elif 'messages' in data:
            return data['messages']
    except:
        pass

    # å°è¯•æŒ‰è¡Œåˆ†å‰²
    lines = content.split('\n')
    if len(lines) > 1:
        return lines

    # å°è¯•æŒ‰å…¶ä»–åˆ†éš”ç¬¦åˆ†å‰²
    for sep in [';', '|', '###', '---']:
        if sep in content:
            return content.split(sep)

    # å¦‚æœéƒ½ä¸è¡Œï¼Œè¿”å›å•è¡Œåˆ—è¡¨
    return [content]


# æ£€æµ‹æ¡ˆä»¶ç±»å‹
def detect_case_type(messages):
    type_counts = defaultdict(int)

    for msg in messages:
        for case_type, keywords in CASE_TYPE_MAPPING.items():
            if any(keyword in msg for keyword in keywords):
                type_counts[case_type] += 1

    if not type_counts:
        return "æœªçŸ¥", {}

    # æŒ‰å‡ºç°é¢‘ç‡æ’åº
    sorted_types = sorted(type_counts.items(), key=lambda x: x[1], reverse=True)
    return sorted_types[0][0], dict(sorted_types)


# ç”Ÿæˆæ¡ˆä»¶æ‘˜è¦
def generate_case_summary(messages):
    # æ£€æµ‹æ¡ˆä»¶ç±»å‹
    primary_type, type_counts = detect_case_type(messages)

    # è¯†åˆ«å…³é”®äººç‰©
    key_persons = defaultdict(int)
    for msg in messages:
        entities = extract_entities(msg)
        for entity, label in entities:
            if label == "PERSON":
                key_persons[entity] += 1

    # è¯†åˆ«å…³é”®åœ°ç‚¹
    locations = defaultdict(int)
    for msg in messages:
        entities = extract_entities(msg)
        for entity, label in entities:
            if label == "GPE":
                locations[entity] += 1

    # é«˜é£é™©æ¶ˆæ¯æ•°é‡
    high_risk_count = 0
    for msg in messages:
        _, risk_score = analyze_criminal_intent(msg)
        if risk_score > 70:
            high_risk_count += 1

    # æ„å»ºæ‘˜è¦
    summary = {
        "æ¡ˆä»¶ç±»å‹": primary_type,
        "ç±»å‹åˆ†å¸ƒ": type_counts,
        "å…³é”®äººç‰©": dict(sorted(key_persons.items(), key=lambda x: x[1], reverse=True)[:3]),
        "å…³é”®åœ°ç‚¹": dict(sorted(locations.items(), key=lambda x: x[1], reverse=True)[:3]),
        "é«˜é£é™©æ¶ˆæ¯": high_risk_count,
        "æ€»æ¶ˆæ¯é‡": len(messages)
    }

    return summary


# ç”ŸæˆPDFæŠ¥å‘Š
def generate_pdf_report(summary, high_risk_messages, entities_count, topic_count, risk_percentage, risk_level):
    # åˆ›å»ºå†…å­˜ç¼“å†²åŒº
    buffer = BytesIO()

    # æ³¨å†Œä¸­æ–‡å­—ä½“ï¼ˆä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸­æ–‡å­—ä½“ï¼‰
    try:
        # å°è¯•æ³¨å†Œä¸­æ–‡å­—ä½“
        pdfmetrics.registerFont(TTFont('SimSun', 'simsun.ttc'))
        font_name = 'SimSun'
    except:
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
        font_name = 'Helvetica'

    # åˆ›å»ºPDFæ–‡æ¡£
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        rightMargin=30,
        leftMargin=30,
        topMargin=30,
        bottomMargin=30
    )

    # å®šä¹‰æ ·å¼
    styles = getSampleStyleSheet()
    title_style = ParagraphStyle(
        'Title',
        parent=styles['Heading1'],
        fontName=font_name,
        fontSize=18,
        alignment=1,  # å±…ä¸­
        spaceAfter=12
    )
    heading_style = ParagraphStyle(
        'Heading',
        parent=styles['Heading2'],
        fontName=font_name,
        fontSize=14,
        spaceAfter=6
    )
    normal_style = ParagraphStyle(
        'Normal',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=12,
        spaceAfter=12
    )
    table_style = TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), font_name),
        ('FONTSIZE', (0, 0), (-1, -1), 10),
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3B71CA')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ])

    # åˆ›å»ºå†…å®¹å…ƒç´ 
    elements = []

    # æ ‡é¢˜
    elements.append(Paragraph("<b>æ¶‰æ¡ˆæ–‡æœ¬æ™ºèƒ½åˆ†ææŠ¥å‘Š</b>", title_style))
    elements.append(Paragraph(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}", normal_style))
    elements.append(Spacer(1, 24))

    # æ¡ˆä»¶æ‘˜è¦
    elements.append(Paragraph("<b>æ¡ˆä»¶æ‘˜è¦</b>", heading_style))
    case_data = [
        ['æ¡ˆä»¶ç±»å‹', summary['æ¡ˆä»¶ç±»å‹']],
        ['æ€»æ¶ˆæ¯é‡', summary['æ€»æ¶ˆæ¯é‡']],
        ['é«˜é£é™©æ¶ˆæ¯', summary['é«˜é£é™©æ¶ˆæ¯']],
        ['è¯†åˆ«å®ä½“æ•°', len(entities_count)],
        ['æ•æ„Ÿè¯é¢˜ç±»å‹', len(topic_count)],
        ['æ•´ä½“é£é™©ç­‰çº§', f"{risk_level} ({risk_percentage:.1f}%)"]
    ]
    case_table = Table(case_data, colWidths=[2 * inch, 3 * inch])
    case_table.setStyle(TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), font_name),
        ('FONTSIZE', (0, 0), (-1, -1), 12),
        ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#E4E6EF')),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    elements.append(case_table)
    elements.append(Spacer(1, 24))

    # å…³é”®ä¿¡æ¯
    elements.append(Paragraph("<b>å…³é”®ä¿¡æ¯</b>", heading_style))

    # å…³é”®äººç‰©
    key_persons = ""
    for person, count in summary['å…³é”®äººç‰©'].items():
        key_persons += f"{person} ({count}æ¬¡), "
    elements.append(Paragraph(f"<b>å…³é”®äººç‰©:</b> {key_persons.rstrip(', ')}", normal_style))

    # å…³é”®åœ°ç‚¹
    key_locations = ""
    for location, count in summary['å…³é”®åœ°ç‚¹'].items():
        key_locations += f"{location} ({count}æ¬¡), "
    elements.append(Paragraph(f"<b>å…³é”®åœ°ç‚¹:</b> {key_locations.rstrip(', ')}", normal_style))
    elements.append(Spacer(1, 12))

    # æ¡ˆä»¶ç±»å‹åˆ†å¸ƒ
    elements.append(Paragraph("<b>æ¡ˆä»¶ç±»å‹åˆ†å¸ƒ</b>", heading_style))
    type_data = [['æ¡ˆä»¶ç±»å‹', 'å‡ºç°æ¬¡æ•°']]
    for case_type, count in summary['ç±»å‹åˆ†å¸ƒ'].items():
        type_data.append([case_type, str(count)])

    type_table = Table(type_data)
    type_table.setStyle(table_style)
    elements.append(type_table)
    elements.append(Spacer(1, 24))

    # é«˜é£é™©æ¶ˆæ¯
    elements.append(Paragraph("<b>é«˜é£é™©æ¶ˆæ¯ (å‰10æ¡)</b>", heading_style))
    risk_data = [['åºå·', 'æ¶ˆæ¯å†…å®¹', 'é£é™©ç­‰çº§']]
    for i, msg in enumerate(high_risk_messages[:10], 1):
        risk_data.append([str(i), msg[:100] + ("..." if len(msg) > 100 else ""), "é«˜å±"])

    risk_table = Table(risk_data, colWidths=[0.5 * inch, 4 * inch, inch])
    risk_table.setStyle(table_style)
    elements.append(risk_table)
    elements.append(Spacer(1, 24))

    # å›¾è¡¨éƒ¨åˆ†
    elements.append(Paragraph("<b>åˆ†æå›¾è¡¨</b>", heading_style))

    # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜å›¾è¡¨
    with tempfile.TemporaryDirectory() as tmpdir:
        # å®ä½“ç»Ÿè®¡å›¾
        plt.figure(figsize=(8, 4))
        if entities_count:
            top_entities = sorted(entities_count.items(), key=lambda x: x[1], reverse=True)[:5]
            df_entities = pd.DataFrame(top_entities, columns=["å®ä½“", "å‡ºç°æ¬¡æ•°"])
            plt.bar(df_entities["å®ä½“"], df_entities["å‡ºç°æ¬¡æ•°"],
                    color=['#ffb6c1', '#add8e6', '#98fb98', '#d9b3ff', '#ffcc99'])
            plt.title("å…³é”®å®ä½“å‡ºç°é¢‘æ¬¡", fontproperties=font_name)
            plt.ylabel("å‡ºç°æ¬¡æ•°", fontproperties=font_name)
            plt.xticks(fontproperties=font_name)
            entity_path = os.path.join(tmpdir, "entities.png")
            plt.savefig(entity_path, bbox_inches='tight')
            plt.close()

            elements.append(Paragraph("<b>å…³é”®å®ä½“ç»Ÿè®¡</b>", normal_style))
            elements.append(Image(entity_path, width=6 * inch, height=3 * inch))
            elements.append(Spacer(1, 12))

        # è¯é¢˜åˆ†å¸ƒå›¾
        plt.figure(figsize=(8, 4))
        if topic_count:
            topic_colors = {
                "æ¯’å“": "#ff9999",
                "è¯ˆéª—": "#ffcc99",
                "æš´åŠ›": "#ff6666",
                "èµŒåš": "#ccccff",
                "èµ°ç§": "#99ccff",
                "æ¶‰é»‘": "#d9b3ff"
            }
            colors1 = [topic_colors.get(topic, "#dddddd") for topic in topic_count.keys()]
            plt.pie(topic_count.values(),
                    labels=topic_count.keys(),
                    colors=colors1,
                    autopct="%1.1f%%",
                    startangle=90,
                    wedgeprops={'edgecolor': 'white', 'linewidth': 1})
            plt.title("æ•æ„Ÿè¯é¢˜åˆ†å¸ƒ", fontproperties=font_name)
            topic_path = os.path.join(tmpdir, "topics.png")
            plt.savefig(topic_path, bbox_inches='tight')
            plt.close()

            elements.append(Paragraph("<b>æ•æ„Ÿè¯é¢˜åˆ†å¸ƒ</b>", normal_style))
            elements.append(Image(topic_path, width=6 * inch, height=3 * inch))

    # é¡µè„š
    elements.append(Spacer(1, 24))
    elements.append(Paragraph("AIæ…§çœ¼ï¼šæ¶‰æ¡ˆæ–‡æœ¬æ™ºèƒ½åˆ†æä¸é¢„è­¦ç³»ç»Ÿ",
                              ParagraphStyle('Footer', parent=normal_style, alignment=1)))
    elements.append(Paragraph("åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šç»´åº¦çŠ¯ç½ªæ„å›¾è¯†åˆ«å¹³å°",
                              ParagraphStyle('Footer', parent=normal_style, alignment=1, fontSize=10)))
    elements.append(Paragraph("ç”Ÿæˆæ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S"),
                              ParagraphStyle('Footer', parent=normal_style, alignment=1, fontSize=9)))

    # ç”ŸæˆPDF
    doc.build(elements)

    # è·å–PDFå†…å®¹
    pdf_data = buffer.getvalue()
    buffer.close()

    return pdf_data

# ä¸»åº”ç”¨
def main():
    st.set_page_config(page_title="AIæ…§çœ¼ï¼šæ¶‰æ¡ˆæ–‡æœ¬æ™ºèƒ½åˆ†æç³»ç»Ÿ", layout="wide")
    st.title("ğŸ•µï¸â€â™‚ï¸ AIæ…§çœ¼ï¼šæ¶‰æ¡ˆæ–‡æœ¬æ™ºèƒ½åˆ†æä¸é¢„è­¦ç³»ç»Ÿ")
    st.caption("åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šç»´åº¦çŠ¯ç½ªæ„å›¾è¯†åˆ«å¹³å° | å¸æ³•æŠ€æœ¯èµ›é¡¹è§£å†³æ–¹æ¡ˆ")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = load_sample_data()
        st.session_state.file_processed = False
        st.session_state.case_summary = None

    # ä¾§è¾¹æ æ§åˆ¶é¢æ¿
    with st.sidebar:
        st.header("ç³»ç»Ÿæ§åˆ¶é¢æ¿")
        analysis_type = st.selectbox("åˆ†ææ¨¡å¼", ["å®æ—¶åˆ†æ", "æ–‡ä»¶åˆ†æ"])
        risk_threshold = st.slider("é£é™©é¢„è­¦é˜ˆå€¼", 0, 100, 70)
        st.divider()

        st.subheader("æ•°æ®ç®¡ç†")
        if st.button("åŠ è½½ç¤ºä¾‹æ•°æ®"):
            st.session_state.messages = load_sample_data()
            st.session_state.file_processed = False
            st.session_state.case_summary = None
            st.success("ç¤ºä¾‹æ•°æ®å·²åŠ è½½!")

        uploaded_file = st.file_uploader("ä¸Šä¼ æ¶‰æ¡ˆæ–‡ä»¶", type=["txt", "json", "csv", "log"])
        if uploaded_file:
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = uploaded_file.read().decode("utf-8")

                # åˆ†ææ–‡ä»¶å†…å®¹
                st.session_state.messages = analyze_file_content(content)
                st.session_state.file_processed = True
                st.session_state.case_summary = generate_case_summary(st.session_state.messages)

                st.success(f"å·²è§£æ {len(st.session_state.messages)} æ¡è®°å½•")
            except Exception as e:
                st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")

        st.divider()
        st.subheader("ç³»ç»Ÿä¿¡æ¯")
        st.info("""
        **æ ¸å¿ƒåŠŸèƒ½æ¶æ„**:
        - å…³é”®å®ä½“è¯†åˆ«
        - æ•æ„Ÿè¯é¢˜èšç±»
        - æ„å›¾é£é™©è¯„åˆ†
        - æƒ…æ„Ÿå€¾å‘åˆ†æ
        - æ¡ˆä»¶ç±»å‹æ£€æµ‹
        """)
        st.markdown("**é«˜äº®é¢œè‰²è¯´æ˜**:")
        st.markdown("- <span style='background-color: #ffb6c1; padding: 2px; border-radius: 3px;'>äººå</span>",
                    unsafe_allow_html=True)
        st.markdown("- <span style='background-color: #add8e6; padding: 2px; border-radius: 3px;'>ç»„ç»‡</span>",
                    unsafe_allow_html=True)
        st.markdown("- <span style='background-color: #98fb98; padding: 2px; border-radius: 3px;'>åœ°ç‚¹</span>",
                    unsafe_allow_html=True)
        st.markdown("- <span style='background-color: #ff9999; padding: 2px; border-radius: 3px;'>æ¯’å“å…³é”®è¯</span>",
                    unsafe_allow_html=True)
        st.markdown("- <span style='background-color: #ffcc99; padding: 2px; border-radius: 3px;'>è¯ˆéª—å…³é”®è¯</span>",
                    unsafe_allow_html=True)
        st.markdown("- <span style='background-color: #ff6666; padding: 2px; border-radius: 3px;'>æš´åŠ›å…³é”®è¯</span>",
                    unsafe_allow_html=True)
        st.markdown("- <span style='background-color: #d9b3ff; padding: 2px; border-radius: 3px;'>æ¶‰é»‘å…³é”®è¯</span>",
                    unsafe_allow_html=True)
        st.markdown(
            "- <span style='background-color: #ffff99; padding: 2px; border-radius: 3px; border: 1px dashed #ff9900;'>çŠ¯ç½ªæš—è¯­</span>",
            unsafe_allow_html=True)

    # ä¸»ç•Œé¢
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["å®æ—¶åˆ†æ", "å…³ç³»å›¾è°±", "èšç±»åˆ†æ", "é¢„è­¦æŠ¥å‘Š", "æ¡ˆä»¶æ‘˜è¦"])

    with tab1:
        st.subheader("å®æ—¶æ–‡æœ¬åˆ†æ")
        input_text = st.text_input("è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†æ", "ä»Šæ™šè€åœ°æ–¹äº¤è´§ï¼Œå¸¦å¤Ÿç°é‡‘")

        if st.button("åˆ†ææ–‡æœ¬", type="primary"):
            with st.spinner("AIåˆ†æä¸­..."):
                # å®ä½“è¯†åˆ«
                entities = extract_entities(input_text)

                # æ•æ„Ÿè¯é¢˜
                topics = detect_sensitive_topics(input_text)

                # æ„å›¾åˆ†æ
                intent, risk_score = analyze_criminal_intent(input_text)

                # æƒ…æ„Ÿåˆ†æ
                sentiment, sentiment_score = analyze_sentiment(input_text)

                # æ˜¾ç¤ºç»“æœ
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.subheader("ğŸ” å®ä½“è¯†åˆ«")
                    if entities:
                        for entity, label in entities:
                            st.info(f"{entity} ({label})")
                    else:
                        st.warning("æœªè¯†åˆ«åˆ°å®ä½“")

                with col2:
                    st.subheader("ğŸ“Œ æ•æ„Ÿè¯é¢˜")
                    if topics:
                        for topic in topics:
                            st.error(f"âš ï¸ {topic}")
                    else:
                        st.success("æœªæ£€æµ‹åˆ°æ•æ„Ÿè¯é¢˜")

                with col3:
                    st.subheader("ğŸ¯ çŠ¯ç½ªæ„å›¾")
                    progress_color = "red" if risk_score > risk_threshold else "green"
                    st.metric("é£é™©è¯„åˆ†", f"{risk_score}%", delta="é«˜é£é™©" if risk_score > risk_threshold else "ä½é£é™©")
                    st.progress(risk_score / 100, text=f"**{intent}**")

                with col4:
                    st.subheader("ğŸ˜ƒ æƒ…æ„Ÿåˆ†æ")
                    if sentiment == "NEGATIVE":
                        st.error(f"è´Ÿé¢æƒ…ç»ª: {sentiment_score:.0%}")
                        st.markdown("<div style='text-align:center; font-size:50px;'>ğŸ”¥</div>", unsafe_allow_html=True)
                    else:
                        st.success(f"æ­£é¢æƒ…ç»ª: {sentiment_score:.0%}")
                        st.markdown("<div style='text-align:center; font-size:50px;'>ğŸ˜Š</div>", unsafe_allow_html=True)

                # é«˜äº®æ˜¾ç¤ºè¾“å…¥æ–‡æœ¬
                st.divider()
                st.subheader("é«˜äº®åˆ†æç»“æœ")
                highlighted_text = highlight_keywords(input_text)
                st.markdown(f"**åŸå§‹æ–‡æœ¬**: {highlighted_text}", unsafe_allow_html=True)

                # æ˜¾ç¤ºæš—è¯­è§£é‡Š
                detected_euphemisms = []
                for euphemism in EUPHEMISMS.keys():
                    if euphemism in input_text:
                        detected_euphemisms.append(f"**{euphemism}** â†’ {EUPHEMISMS[euphemism]}")

                if detected_euphemisms:
                    st.info("æ£€æµ‹åˆ°çŠ¯ç½ªæš—è¯­: " + " | ".join(detected_euphemisms))

    with tab2:
        st.subheader("çŠ¯ç½ªç½‘ç»œå…³ç³»å›¾è°±")

        if st.button("ç”Ÿæˆå…³ç³»å›¾è°±", key="graph_btn"):
            with st.spinner("æ„å»ºçŠ¯ç½ªç½‘ç»œ..."):
                G, entity_messages = build_relationship_graph(st.session_state.messages)

                # è®¡ç®—èŠ‚ç‚¹å¤§å°ï¼ˆåŸºäºä¸­å¿ƒåº¦ï¼‰
                node_sizes = [G.nodes[n]['centrality'] * 3000 + 500 for n in G.nodes()]

                # ç»˜åˆ¶å›¾è°±
                plt.figure(figsize=(12, 8))
                pos = nx.spring_layout(G, seed=42)

                # ç»˜åˆ¶èŠ‚ç‚¹
                nx.draw_networkx_nodes(G, pos, node_size=node_sizes,
                                       node_color="lightcoral", alpha=0.9)

                # ç»˜åˆ¶è¾¹
                nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color="gray")

                # ç»˜åˆ¶æ ‡ç­¾
                nx.draw_networkx_labels(G, pos, font_size=12, font_family="SimHei")

                plt.title("çŠ¯ç½ªç½‘ç»œå…³ç³»å›¾è°±", fontsize=16)
                plt.axis("off")
                st.pyplot(plt)

                # æ˜¾ç¤ºèŠ‚ç‚¹è¯¦æƒ…
                selected_node = st.selectbox("é€‰æ‹©èŠ‚ç‚¹æŸ¥çœ‹è¯¦æƒ…", list(G.nodes()))

                if selected_node:
                    st.subheader(f"èŠ‚ç‚¹: {selected_node}")

                    # é‚»å±…èŠ‚ç‚¹
                    neighbors = list(G.neighbors(selected_node))
                    st.write(f"**å…³è”èŠ‚ç‚¹**: {', '.join(neighbors) if neighbors else 'æ— '}")

                    # ä¸­å¿ƒåº¦
                    centrality = G.nodes[selected_node].get('centrality', 0)
                    st.metric("èŠ‚ç‚¹å½±å“åŠ›", f"{centrality * 100:.1f}%")

                    # ç›¸å…³æ¶ˆæ¯
                    related_msgs = []
                    for neighbor in neighbors:
                        if (selected_node, neighbor) in entity_messages:
                            related_msgs.extend(entity_messages[(selected_node, neighbor)])

                    if related_msgs:
                        st.subheader("ç›¸å…³æ¶ˆæ¯")
                        for msg in set(related_msgs):
                            # é«˜äº®æ˜¾ç¤ºæ¶ˆæ¯
                            highlighted_msg = highlight_keywords(msg)
                            st.markdown(f"- {highlighted_msg}", unsafe_allow_html=True)
                    else:
                        st.info("æœªæ‰¾åˆ°ç›¸å…³æ¶ˆæ¯")

    with tab3:
        st.subheader("æ•æ„Ÿè¯é¢˜èšç±»åˆ†æ")
        n_clusters = st.slider("èšç±»æ•°é‡", 2, 6, 4)

        if st.button("æ‰§è¡Œèšç±»åˆ†æ", key="cluster_btn"):
            with st.spinner("èšç±»åˆ†æä¸­..."):
                clusters = cluster_messages(st.session_state.messages, n_clusters)

                # æ˜¾ç¤ºèšç±»ç»“æœ
                for cluster_id, messages in clusters.items():
                    with st.expander(f"è¯é¢˜èšç±» #{cluster_id + 1} (å…±{len(messages)}æ¡)"):
                        for msg in messages:
                            # é«˜äº®å…³é”®è¯
                            highlighted_msg = highlight_keywords(msg)
                            st.markdown(f"- {highlighted_msg}", unsafe_allow_html=True)

    with tab4:
        st.subheader("çŠ¯ç½ªé¢„è­¦æŠ¥å‘Š")

        if st.button("ç”Ÿæˆé¢„è­¦æŠ¥å‘Š", type="primary", key="report_btn"):
            with st.spinner("åˆ†ææ‰€æœ‰æ¶ˆæ¯å¹¶ç”ŸæˆæŠ¥å‘Š..."):
                # åˆ†ææ‰€æœ‰æ¶ˆæ¯
                high_risk = []
                entities_count = defaultdict(int)
                topic_count = defaultdict(int)

                for msg in st.session_state.messages:
                    _, risk_score = analyze_criminal_intent(msg)
                    if risk_score > risk_threshold:
                        high_risk.append(msg)

                    # ç»Ÿè®¡å®ä½“
                    for entity, _ in extract_entities(msg):
                        entities_count[entity] += 1

                    # ç»Ÿè®¡è¯é¢˜
                    topics = detect_sensitive_topics(msg)
                    for topic in topics:
                        topic_count[topic] += 1

                # æ˜¾ç¤ºé«˜é£é™©æ¶ˆæ¯
                st.subheader(f"é«˜é£é™©æ¶ˆæ¯ ({len(high_risk)}æ¡)")
                if high_risk:
                    for msg in high_risk:
                        highlighted_msg = highlight_keywords(msg)
                        st.error(f"âš ï¸ {highlighted_msg}", icon="ğŸ”¥")
                else:
                    st.success("æœªæ£€æµ‹åˆ°é«˜é£é™©æ¶ˆæ¯")

                # å®ä½“ç»Ÿè®¡
                st.subheader("å…³é”®å®ä½“ç»Ÿè®¡")
                if entities_count:
                    top_entities = sorted(entities_count.items(), key=lambda x: x[1], reverse=True)[:5]
                    df_entities = pd.DataFrame(top_entities, columns=["å®ä½“", "å‡ºç°æ¬¡æ•°"])

                    # æ·»åŠ é¢œè‰²æ˜ å°„
                    colors1 = []
                    for entity in df_entities["å®ä½“"]:
                        # æ ¹æ®å®ä½“ç±»å‹åˆ†é…é¢œè‰²
                        entity_type = ""
                        for msg in st.session_state.messages:
                            if entity in msg:
                                entities_in_msg = extract_entities(msg)
                                for e, t in entities_in_msg:
                                    if e == entity:
                                        entity_type = t
                                        break
                                if entity_type:
                                    break

                        if entity_type == "PERSON":
                            colors1.append("#ffb6c1")
                        elif entity_type == "ORG":
                            colors1.append("#add8e6")
                        elif entity_type == "GPE":
                            colors1.append("#98fb98")
                        else:
                            colors1.append("#cccccc")

                    fig, ax = plt.subplots()
                    bars = ax.bar(df_entities["å®ä½“"], df_entities["å‡ºç°æ¬¡æ•°"], color=colors1)
                    ax.set_title("å…³é”®å®ä½“å‡ºç°é¢‘æ¬¡")
                    ax.set_ylabel("å‡ºç°æ¬¡æ•°")
                    plt.xticks(rotation=45)


                    # æ·»åŠ å›¾ä¾‹
                    from matplotlib.patches import Patch
                    legend_elements = [
                        Patch(facecolor='#ffb6c1', label='äººç‰©'),
                        Patch(facecolor='#add8e6', label='ç»„ç»‡'),
                        Patch(facecolor='#98fb98', label='åœ°ç‚¹')
                    ]
                    ax.legend(handles=legend_elements, loc='upper right')

                    st.pyplot(fig)
                else:
                    st.info("æœªè¯†åˆ«åˆ°å…³é”®å®ä½“")

                # è¯é¢˜ç»Ÿè®¡
                st.subheader("æ•æ„Ÿè¯é¢˜åˆ†å¸ƒ")
                if topic_count:
                    # åˆ†é…é¢œè‰²
                    topic_colors = {
                        "æ¯’å“": "#ff9999",
                        "è¯ˆéª—": "#ffcc99",
                        "æš´åŠ›": "#ff6666",
                        "èµŒåš": "#ccccff",
                        "èµ°ç§": "#99ccff",
                        "æ¶‰é»‘": "#d9b3ff"
                    }

                    colors = [topic_colors.get(topic, "#dddddd") for topic in topic_count.keys()]

                    fig, ax = plt.subplots()
                    ax.pie(topic_count.values(),
                           labels=topic_count.keys(),
                           colors=colors,
                           autopct="%1.1f%%",
                           startangle=90,
                           wedgeprops={'edgecolor': 'white', 'linewidth': 1})
                    ax.axis("equal")
                    ax.set_title("æ•æ„Ÿè¯é¢˜åˆ†å¸ƒ")
                    st.pyplot(fig)
                else:
                    st.info("æœªæ£€æµ‹åˆ°æ•æ„Ÿè¯é¢˜")

                # ç”ŸæˆæŠ¥å‘Šæ‘˜è¦
                st.divider()
                st.subheader("åˆ†ææŠ¥å‘Šæ‘˜è¦")
                col1, col2 = st.columns(2)

                with col1:
                    st.metric("æ€»æ¶ˆæ¯é‡", len(st.session_state.messages))
                    st.metric("é«˜é£é™©æ¶ˆæ¯", len(high_risk))

                with col2:
                    st.metric("è¯†åˆ«å®ä½“æ•°", len(entities_count))
                    st.metric("æ•æ„Ÿè¯é¢˜ç±»å‹", len(topic_count))

                # é£é™©ç­‰çº§è¯„ä¼°
                risk_percentage = len(high_risk) / len(st.session_state.messages) * 100
                risk_level = "é«˜å±" if risk_percentage > 20 else "ä¸­å±" if risk_percentage > 5 else "ä½å±"

                # æ ¹æ®é£é™©ç­‰çº§è®¾ç½®é¢œè‰²
                if risk_level == "é«˜å±":
                    color = "#ff4b4b"
                elif risk_level == "ä¸­å±":
                    color = "#ffa700"
                else:
                    color = "#0f9d58"

                st.markdown(f"**æ•´ä½“é£é™©ç­‰çº§: <span style='color:{color};'>{risk_level}</span> ({risk_percentage:.1f}%)**", unsafe_allow_html=True, help=f"é£é™©ç­‰çº§è¯„ä¼°æ ‡å‡†: >20%é«˜å±, >5%ä¸­å±, â‰¤5%ä½å±")
                st.progress(risk_percentage/100)
                # st.progress(risk_percentage/100, text=f"**æ•´ä½“é£é™©ç­‰çº§: <span style='color:{color};'>{risk_level}</span> ({risk_percentage:.1f}%)**")
                # st.progress(
                #     risk_percentage / 100,
                #     help="é£é™©ç­‰çº§è¯„ä¼°æ ‡å‡†: >20%é«˜å±, >5%ä¸­å±, â‰¤5%ä½å±",
                #     label=f"**æ•´ä½“é£é™©ç­‰çº§: :{color}[{risk_level}] ({risk_percentage:.1f}%)**"  # ä½¿ç”¨ Markdown ç€è‰²
                # )

    with tab5:
        st.subheader("æ¡ˆä»¶æ‘˜è¦åˆ†æ")

        if st.session_state.file_processed and st.session_state.case_summary:
            summary = st.session_state.case_summary

            # æ¡ˆä»¶ç±»å‹å±•ç¤º
            st.subheader(f"æ¡ˆä»¶ç±»å‹: {summary['æ¡ˆä»¶ç±»å‹']}")

            # ç±»å‹åˆ†å¸ƒ
            st.subheader("çŠ¯ç½ªç±»å‹åˆ†å¸ƒ")
            type_df = pd.DataFrame(list(summary['ç±»å‹åˆ†å¸ƒ'].items()), columns=['ç±»å‹', 'å‡ºç°æ¬¡æ•°'])
            st.bar_chart(type_df.set_index('ç±»å‹'))


            # å…³é”®äººç‰©
            st.subheader("å…³é”®äººç‰©")
            if summary['å…³é”®äººç‰©']:
                for person, count in summary['å…³é”®äººç‰©'].items():
                    st.markdown(f"- **{person}**: åœ¨ {count} æ¡æ¶ˆæ¯ä¸­å‡ºç°")
            else:
                st.info("æœªè¯†åˆ«åˆ°å…³é”®äººç‰©")

            # å…³é”®åœ°ç‚¹
            st.subheader("å…³é”®åœ°ç‚¹")
            if summary['å…³é”®åœ°ç‚¹']:
                for location, count in summary['å…³é”®åœ°ç‚¹'].items():
                    st.markdown(f"- **{location}**: åœ¨ {count} æ¡æ¶ˆæ¯ä¸­å‡ºç°")
            else:
                st.info("æœªè¯†åˆ«åˆ°å…³é”®åœ°ç‚¹")

            # é£é™©ç»Ÿè®¡
            st.subheader("é£é™©ç»Ÿè®¡")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("é«˜é£é™©æ¶ˆæ¯", summary['é«˜é£é™©æ¶ˆæ¯'])
            with col2:
                st.metric("æ€»æ¶ˆæ¯é‡", summary['æ€»æ¶ˆæ¯é‡'])

            if st.button("ç”Ÿæˆæ¡ˆä»¶æ‘˜è¦æŠ¥å‘Š", type="primary"):
                with st.spinner("ç”ŸæˆPDFæŠ¥å‘Šä¸­..."):
                    # é‡æ–°è®¡ç®—é«˜é£é™©æ¶ˆæ¯ï¼ˆä½¿ç”¨å½“å‰é˜ˆå€¼ï¼‰
                    high_risk = []
                    for msg in st.session_state.messages:
                        _, risk_score = analyze_criminal_intent(msg)
                        if risk_score > risk_threshold:
                            high_risk.append(msg)

                    # é‡æ–°è®¡ç®—å®ä½“å’Œè¯é¢˜ç»Ÿè®¡
                    entities_count = defaultdict(int)
                    topic_count = defaultdict(int)
                    for msg in st.session_state.messages:
                        # ç»Ÿè®¡å®ä½“
                        for entity, _ in extract_entities(msg):
                            entities_count[entity] += 1
                        # ç»Ÿè®¡è¯é¢˜
                        topics = detect_sensitive_topics(msg)
                        for topic in topics:
                            topic_count[topic] += 1

                    # è®¡ç®—é£é™©ç™¾åˆ†æ¯”
                    risk_percentage = len(high_risk) / len(st.session_state.messages) * 100
                    risk_level = "é«˜å±" if risk_percentage > 20 else "ä¸­å±" if risk_percentage > 5 else "ä½å±"

                    # ç”ŸæˆPDF
                    pdf_data = generate_pdf_report(
                        summary,
                        high_risk,
                        entities_count,
                        topic_count,
                        risk_percentage,
                        risk_level
                        )

                    st.success("æ¡ˆä»¶æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆï¼")
                    st.download_button(
                        label="ä¸‹è½½æ¡ˆä»¶æ‘˜è¦æŠ¥å‘Š (PDF)",
                        data=pdf_data,
                        file_name="AIæ…§çœ¼-æ¡ˆä»¶åˆ†ææŠ¥å‘Š.pdf",
                        mime="application/pdf"
                    )
        else:
            st.info("è¯·ä¸Šä¼ æ¶‰æ¡ˆæ–‡ä»¶ä»¥ç”Ÿæˆæ¡ˆä»¶æ‘˜è¦")

if __name__ == "__main__":
    main()